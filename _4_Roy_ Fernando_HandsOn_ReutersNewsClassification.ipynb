{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roy392/AiPinter-v2.0/blob/master/_4_Roy_%20Fernando_HandsOn_ReutersNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuE8Vksj6umX"
      },
      "source": [
        "**Classifying newswires: a multiclass classification example**\n",
        "The Reuters dataset\n",
        "You’ll work with the Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKrmmiKu64Y_"
      },
      "source": [
        "Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUlcqB1z6t0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028400ec-1a7a-4505-9506-1e0a54dea453"
      },
      "source": [
        "#Import dataset\n",
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXCVMjN8_idx"
      },
      "source": [
        "#Check the train data and train labels, Write your comment/review on the those train data n train label\n",
        "#Write your code here, put your comment\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRUV49YgzMhi"
      },
      "source": [
        "**Decoding newswires back to text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC6p77UQzRck"
      },
      "source": [
        "word_index = reuters.get_word_index() \n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1JFdM0NztIb"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7y8XfMYzrpz"
      },
      "source": [
        "import numpy as np\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension)) \n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data) \n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "OCgKeYPd1zDY",
        "outputId": "8b2d8495-8e17-491a-a617-c6a9b8c078bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bFXJZJc0Fsg"
      },
      "source": [
        "To vectorize the labels, there are two possibilities: you can cast the label list as an integer tensor, or you can use one-hot encoding. One-hot encoding is a widely used format for categorical data, also called categorical encoding. For a more detailed explanation of one-hot encoding, see section 6.1. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. Here’s an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONb_0vov0IPC"
      },
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "  results = np.zeros((len(labels), dimension)) \n",
        "  for i, label in enumerate(labels):\n",
        "    results[i, label] = 1.\n",
        "  return results\n",
        "one_hot_train_labels = to_one_hot(train_labels) \n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ9OLPhJ1DPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2843a055-2b03-408a-f404-c4fd89f54c57"
      },
      "source": [
        "#Check one_hot_train_labels\n",
        "#Code your write here\n",
        "one_hot_train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p5LfmkY07ja"
      },
      "source": [
        "Note that there is a built-in way to do this in Keras, which you’ve already seen in action in the MNIST example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZOzrVy70-B6"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels) \n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2xI6G3g1JDd"
      },
      "source": [
        "#Check one_hot_train_labels\n",
        "#Code your write here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HacIkpQc1PHw"
      },
      "source": [
        "**Building your network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2waLuhd1bwR"
      },
      "source": [
        "In the previous example, you used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\n",
        "For this reason you’ll use larger layers. Let’s go with 64 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3WxfUzv1fiw"
      },
      "source": [
        "#Model definition\n",
        "from keras import models \n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential() \n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiiIvkjDJodO"
      },
      "source": [
        "x_val = x_train[:10000] \n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = one_hot_train_labels[:10000] \n",
        "partial_y_train =one_hot_train_labels[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf-A3qWQ1n4P",
        "outputId": "3c8ba25c-3b8d-499d-ded7-0e257840fbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZouGM8h1q1V"
      },
      "source": [
        "**Compiling the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6ibQzwV1syL"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "  loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S52HDub01-6f"
      },
      "source": [
        "**Validating The Approach**\n",
        "Let’s set apart 1,000 samples in the training data to use as a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpXt4k-_KV28"
      },
      "source": [
        "Training your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "kwrJWZSRKWpc",
        "outputId": "a54ee199-ad5f-43b6-9d8e-c1716f508264"
      },
      "source": [
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-be6266211430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuGmkboRKroE"
      },
      "source": [
        "Plotting the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFv44FA7KxWh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss') \n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss') \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdsOvFiMZwQ"
      },
      "source": [
        "Plotting the training and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovjyPbF43109"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7TdbDQWMats"
      },
      "source": [
        "plt.clf() \n",
        "#Clears the figure \n",
        "acc = history.history['accuracy'] \n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss') \n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlbGLZmP4y3f"
      },
      "source": [
        "The network begins to overfit after nine epochs. Let’s train a new network from scratch for nine epochs and then evaluate it on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScSwG6fY4n50"
      },
      "source": [
        "**Retraining a model from scratch**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1pwLvzt43nQ"
      },
      "source": [
        "model = models.Sequential() \n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val)) \n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdGbo9K6R7KK"
      },
      "source": [
        "Using a trained network to generate predictions on new data\n",
        "You can verify that the predict method of the model instance returns a probability distribution over all 46 topics. Let’s generate topic predictions for all of the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MurDebq-R9Bk"
      },
      "source": [
        "predictions=model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "tduBlSA4U4bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "vZVJe9ibg0Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Menghitung akurasi hasil prediksi model \n",
        "from tensorflow.keras import metrics\n",
        "m = metrics.Accuracy()\n",
        "m.update_state( predictions.argmax(axis=1), one_hot_test_labels.argmax(axis=1))\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "id": "nTlZQbFbgfMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "wGF4nl60z1tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predictions.argmax(axis=1)), one_hot_test_labels.argmax(axis=1))#, target_names=class_names))\n",
        "print(confusion_matrix(predictions.argmax(axis=1), one_hot_test_labels.argmax(axis=1)))"
      ],
      "metadata": {
        "id": "jEqP8eiXz-am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSRMyM_7z1MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztqgfFjWSLrt"
      },
      "source": [
        "**Further experiments**\n",
        "\n",
        "Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
        "\n",
        "1.   You used two hidden layers. \n",
        "2.   Now try using a single hidden layer, or three hidden layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnzT-dW0SqPO"
      },
      "source": [
        "#HANDS ON...\n",
        "#Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}